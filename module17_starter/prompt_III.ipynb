{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3243041851.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[40], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    To predict whether a client will subscribe to a term deposit on the bank clinet data, contact information, campaign details and socio-economic context\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "To predict whether a client will subscribe to a term deposit on the bank clinet data, contact information, campaign details and socio-economic context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3086631777.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[41], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    taget variable y having yes or no will indicate whether the client is subscribed to a term deposit\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "taget variable y having yes or no will indicate whether the client is subscribed to a term deposit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features, prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def prepare_feat (df):\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    df_processed['previously_contacted'] = (df_processed['pdays'] !=999).astype(int)\n",
    "\n",
    "    df_processed.loc[df_processed['pdays'] == 999, 'pdays' ] = np.nan\n",
    "\n",
    "    categorical_cols = ['job', 'marital','education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "    numerical_cols = ['age', 'duration', 'campaign','previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx',\n",
    "                      'euribor3m', 'nr.employed', 'previously_contacted']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_cols)\n",
    "        ])\n",
    "\n",
    "    df_processed['y'] = df_processed['y'].map({'yes': 1, 'no': 0 })\n",
    "\n",
    "    return df_processed, preprocessor\n",
    "\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y'].map({'yes': 1, 'no': 0 })\n",
    "\n",
    "X_processed, preprocessor = prepare_feat(df)\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X_processed.drop('y', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "def prepare_and_split(df):\n",
    "    data = df.copy()\n",
    "\n",
    "    data['y'] = data['y'].map({'yes': 1, 'no': 0 })\n",
    "\n",
    "    data['previously_contacted'] = (data['pdays'] !=999).astype(int)\n",
    "    data.loc[data['pdays'] == 999, 'pdays'] = np.nan\n",
    "\n",
    "    X = data.drop('y', axis =1)\n",
    "    y = data['y']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def preprocessing_pipeline(X_train):\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline(y_train):\n",
    "    \"\"\"\n",
    "    Establish a baseline performance based on majority class classifier\n",
    "    \"\"\"\n",
    "    print(\"\\nProblem 7: Establishing a baseline model\")\n",
    "    \n",
    "    # Most frequent class in the training set\n",
    "    majority_class = pd.Series(y_train).value_counts().idxmax()\n",
    "    baseline_accuracy = (y_train == majority_class).mean()\n",
    "    \n",
    "    print(f\"Majority class: {majority_class}\")\n",
    "    print(f\"Baseline accuracy (always predicting the majority class): {baseline_accuracy:.4f}\")\n",
    "    \n",
    "    return baseline_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def build_logistic_regression(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Build a logistic regression model\n",
    "    \"\"\"\n",
    "    print(\"\\nProblem 8: Building a Logistic Regression model\")\n",
    "    \n",
    "    # Create the pipeline with preprocessing and logistic regression\n",
    "    log_reg_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Train the model and time it\n",
    "    start_time = time()\n",
    "    log_reg_pipeline.fit(X_train, y_train)\n",
    "    train_time = time() - start_time\n",
    "    \n",
    "    # Make predictions on train and test sets\n",
    "    y_train_pred = log_reg_pipeline.predict(X_train)\n",
    "    y_test_pred = log_reg_pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Logistic Regression - Training time: {train_time:.4f} seconds\")\n",
    "    print(f\"Logistic Regression - Training accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Logistic Regression - Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Return the model and metrics\n",
    "    return log_reg_pipeline, train_time, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model with more detailed metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nProblem 9: Detailed model evaluation\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Create and print classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No', 'Yes'],\n",
    "                yticklabels=['No', 'Yes'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    from sklearn.metrics import RocCurveDisplay\n",
    "    RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
    "    plt.grid(True)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|     |    |.     |.     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Compare different machine learning models\n",
    "    \"\"\"\n",
    "    print(\"\\nProblem 10: Comparing different models\")\n",
    "    \n",
    "    # Dictionary to store models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Results DataFrame\n",
    "    results = pd.DataFrame(columns=['Model', 'Train Time', 'Train Accuracy', 'Test Accuracy'])\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        # Train the model and time it\n",
    "        start_time = time()\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        train_time = time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = pipeline.predict(X_train)\n",
    "        y_test_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results = pd.concat([results, pd.DataFrame({\n",
    "            'Model': [name],\n",
    "            'Train Time': [f\"{train_time:.4f}s\"],\n",
    "            'Train Accuracy': [f\"{train_accuracy:.4f}\"],\n",
    "            'Test Accuracy': [f\"{test_accuracy:.4f}\"]\n",
    "        })], ignore_index=True)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Model Comparison Results:\")\n",
    "    print(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "- More feature engineering and exploration.  For example, should we keep the gender feature?  Why or why not?\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def improve_model(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Improve the model through hyperparameter tuning and feature engineering\n",
    "    \"\"\"\n",
    "    print(\"\\nProblem 11: Improving the model\")\n",
    "    \n",
    "    # 1. Feature Importance Analysis\n",
    "    # Let's use a Random Forest to assess feature importance\n",
    "    print(\"Analyzing feature importance with Random Forest...\")\n",
    "    \n",
    "    # Train a Random Forest\n",
    "    rf_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature names from the preprocessor\n",
    "    # This is complex due to OneHotEncoder creating multiple columns\n",
    "    cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Get onehotencoder categories\n",
    "    cat_features = []\n",
    "    try:\n",
    "        ohe_step = rf_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder']\n",
    "        for i, col in enumerate(cat_cols):\n",
    "            categories = ohe_step.categories_[i]\n",
    "            for cat in categories:\n",
    "                cat_features.append(f\"{col}_{cat}\")\n",
    "    except:\n",
    "        # Simplified approach if the above fails\n",
    "        for col in cat_cols:\n",
    "            cat_features.append(col)\n",
    "    \n",
    "    feature_names = num_cols + cat_features\n",
    "    \n",
    "    # Get feature importances (limit to the number of actual features)\n",
    "    if hasattr(rf_pipeline.named_steps['classifier'], 'feature_importances_'):\n",
    "        importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "        if len(importances) == len(feature_names):\n",
    "            # Create a DataFrame of feature importances\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(\"Top 10 Most Important Features:\")\n",
    "            print(feature_importance_df.head(10))\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15))\n",
    "            plt.title('Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('feature_importance.png')\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"Feature names and importances dimension mismatch - skipping detailed feature importance\")\n",
    "    else:\n",
    "        print(\"Model doesn't provide feature importances - skipping feature importance analysis\")\n",
    "    \n",
    "    # 2. Hyperparameter Tuning\n",
    "    print(\"\\nPerforming hyperparameter tuning...\")\n",
    "    \n",
    "    # Grid search for Random Forest\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    # To save time, we'll use a simplified grid\n",
    "    rf_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(rf_pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate the tuned model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Tuned Random Forest - Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # 3. More detailed evaluation of the best model\n",
    "    print(\"\\nDetailed evaluation of the best model:\")\n",
    "    final_accuracy, final_precision, final_recall, final_f1, final_roc_auc = evaluate_model(best_model, X_test, y_test)\n",
    "    \n",
    "    return best_model, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem 7: Establishing a baseline model\n",
      "Majority class: 0\n",
      "Baseline accuracy (always predicting the majority class): 0.8873\n",
      "\n",
      "Problem 8: Building a Logistic Regression model\n",
      "Logistic Regression - Training time: 0.2607 seconds\n",
      "Logistic Regression - Training accuracy: 0.9100\n",
      "Logistic Regression - Test accuracy: 0.9165\n",
      "\n",
      "Problem 9: Detailed model evaluation\n",
      "Accuracy: 0.9165\n",
      "Precision: 0.7120\n",
      "Recall: 0.4343\n",
      "F1 Score: 0.5395\n",
      "ROC AUC: 0.9425\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      7310\n",
      "           1       0.71      0.43      0.54       928\n",
      "\n",
      "    accuracy                           0.92      8238\n",
      "   macro avg       0.82      0.71      0.75      8238\n",
      "weighted avg       0.91      0.92      0.91      8238\n",
      "\n",
      "\n",
      "Problem 10: Comparing different models\n",
      "Model Comparison Results:\n",
      "                 Model Train Time Train Accuracy Test Accuracy\n",
      "0  Logistic Regression    0.3260s         0.9100        0.9165\n",
      "1                  KNN    0.1407s         0.9281        0.9079\n",
      "2        Decision Tree    0.2714s         1.0000        0.8968\n",
      "3                  SVM  110.6186s         0.9219        0.9160\n",
      "\n",
      "Problem 11: Improving the model\n",
      "Analyzing feature importance with Random Forest...\n",
      "Top 10 Most Important Features:\n",
      "                 Feature  Importance\n",
      "1               duration    0.280324\n",
      "8              euribor3m    0.095152\n",
      "0                    age    0.079553\n",
      "9            nr.employed    0.053111\n",
      "2               campaign    0.039339\n",
      "7          cons.conf.idx    0.025004\n",
      "10  previously_contacted    0.024455\n",
      "5           emp.var.rate    0.020392\n",
      "6         cons.price.idx    0.019922\n",
      "63      poutcome_success    0.017885\n",
      "\n",
      "Performing hyperparameter tuning...\n",
      "Best parameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Best cross-validation score: 0.9120\n",
      "Tuned Random Forest - Test accuracy: 0.9207\n",
      "\n",
      "Detailed evaluation of the best model:\n",
      "\n",
      "Problem 9: Detailed model evaluation\n",
      "Accuracy: 0.9207\n",
      "Precision: 0.7186\n",
      "Recall: 0.4871\n",
      "F1 Score: 0.5806\n",
      "ROC AUC: 0.9497\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      7310\n",
      "           1       0.72      0.49      0.58       928\n",
      "\n",
      "    accuracy                           0.92      8238\n",
      "   macro avg       0.83      0.73      0.77      8238\n",
      "weighted avg       0.91      0.92      0.91      8238\n",
      "\n",
      "\n",
      "Summary of Findings:\n",
      "Baseline Accuracy: 0.8873\n",
      "Logistic Regression Accuracy: 0.9165\n",
      "Best Model Accuracy: 0.9207\n",
      "Improvement over baseline: 3.34%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "def bank_marketing_analysis(df):\n",
    "    \"\"\"\n",
    "    Run the entire analysis pipeline\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test, processed_data = prepare_and_split(df)\n",
    "    preprocessor = preprocessing_pipeline(X_train)\n",
    "    \n",
    "    baseline_accuracy = create_baseline(y_train)\n",
    "    \n",
    "    log_reg_model, log_reg_time, log_reg_train_acc, log_reg_test_acc = build_logistic_regression(\n",
    "        X_train, X_test, y_train, y_test, preprocessor\n",
    "    )\n",
    "    \n",
    "    log_reg_accuracy, log_reg_precision, log_reg_recall, log_reg_f1, log_reg_roc_auc = evaluate_model(\n",
    "        log_reg_model, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    model_comparison_results = compare_models(X_train, X_test, y_train, y_test, preprocessor)\n",
    "    \n",
    "    best_model, best_model_accuracy = improve_model(X_train, X_test, y_train, y_test, preprocessor)\n",
    "    \n",
    "    print(\"\\nSummary of Findings:\")\n",
    "    print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "    print(f\"Logistic Regression Accuracy: {log_reg_accuracy:.4f}\")\n",
    "    print(f\"Best Model Accuracy: {best_model_accuracy:.4f}\")\n",
    "    print(f\"Improvement over baseline: {(best_model_accuracy - baseline_accuracy) * 100:.2f}%\")\n",
    "    \n",
    "    return processed_data, best_model, model_comparison_results\n",
    "\n",
    "\n",
    "processed_data, best_model, model_comparison_results = bank_marketing_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
